{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q. What do we mean by we are continually updating our beliefs(prior) with each new experience of the world?\n",
    "\n",
    "Update a prior in bayesian inference means that we start with some guesses about the probability of an event occuring (prior probability), then we observe what happend (likelihood), and depending on what happened we update initial guess. Once updated, our prior probability is called posterior probability.\n",
    "\n",
    "Of course, now we can:\n",
    "1. Stop with our posterior probability.\n",
    "2. Use our posterior probability as a new prior, and update such a probability to obtain a new posterior by observing more evidence (i.e data).\n",
    "\n",
    "Essentially, updating a prior means that we start with a (informed) guess and we use evidence to update our initial guess. \n",
    "\n",
    "Recall that\n",
    "$ p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)} $\n",
    "\n",
    "where $p(\\theta)$ is your prior, $p(x|\\theta)$ is the likelihood (i.e. the evidence that you use to update the prior), and $p(\\theta|x)$ is the posterior probability. Notice that the posterior probability is a probability given the evidence. \n",
    "\n",
    "**Example of coins:**\n",
    "\n",
    "Assume a coin which is 0.1 probable to be unfair. So our prior probability on the coin being unfair is 0.1, and being fair is 0.9. Also by unfair, the probability of head is 2/3 instead of 1/2. Now, imagine a scenario where we have to toss this coin 10 times and we get 10 heads.\n",
    "\n",
    "Here we start with the guess the probability of the coin being fair is p=0.1. Then, we toss 10 times the coin, and we obtain a posterior probability p=0.34. At this point, we can decide to be satistifed with p=0.34 or toss the coin again (say 90 times): in this case, our prior will be p=0.34 i.e the posterior becomes the new prior and we will obtain a new posterior probability depending on new evidence.\n",
    "\n",
    "Suppose that after 1000 tosses our posterior probability is p=0.9. At the beginning our prior was p=0.1, so we were supposing that the coin was unfair. Now based on the evidence of 1000 tosses, we see that the probability of the coin being fair is high. \n",
    "\n",
    "Notice that the fact that we can easily update a probability as we have new evidences is a strength of the bayesian framework. The point here is not that the prior must be updated, but to use all the available evidence to update our guess about a certain probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
